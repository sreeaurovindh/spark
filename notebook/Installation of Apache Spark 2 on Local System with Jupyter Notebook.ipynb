{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href=\"http://nbviewer.jupyter.org/github/sreeaurovindh/spark/blob/master/notebook/Behavior%20Modelling%20with%20Apache%20Spark.ipynb\">[Back to Main Page]</a>        <a href=\"http://www.sreeml.com/folio.html\">[Back to Portfolio]</a>   \n",
    "\n",
    "# Introduction\n",
    "\n",
    "This notebook provides a basic introduction to Spark and describes various RDD opearations that can be performed on the Spark Machine. It also provides installation instructions on how to operate Spark 2 with Jupyter Notebook on a single node cluster. The operating system used for this example is Ubuntu 16.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is a general purpose data processing engine that is suitable for application developers to rapidly query, analyze and transform data at scale. These include interactive queries across a wide scale. It works on a paradigm of data flow using Resilent Districtuted \n",
    "\n",
    "Spark is often used along with HDFS (Hadoop Distributed File System)\n",
    "\n",
    "Spark can deal:\n",
    "* Streaming and Processing of Information\n",
    "* Machine Learning\n",
    "    - Sparks ability to store data in memory and repeatedly run repeated query makes it a good choice of training\n",
    "      machine learning algorithnms\n",
    "    - Running a query again and again makes it more efficient as it is stored in memory\n",
    "* Interactive Streaming analytics\n",
    "    - Ask Questions\n",
    "    - View Results\n",
    "* Data Integration \n",
    "    - ETL processing\n",
    "\n",
    "\n",
    "Resilient distributed datasets (RDDs)\n",
    "* »Immutable collections partitioned across cluster that can be rebuilt if a partition is lost\n",
    "* Created by transforming data in stable storage using data flow operators (map, filter, group-by, …)\n",
    "* Can be cached across parallel operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of RDD Operations\n",
    "\n",
    "1. Transformations [ They define a new RDD] \n",
    "    They include\n",
    "    - map\n",
    "    - filter\n",
    "    - sample\n",
    "    - union\n",
    "    - groupByKey\n",
    "    - reduceByKey\n",
    "    - join\n",
    "    - cache\n",
    "    \n",
    "2. Parallel Operations [ They return a result to the driver] \n",
    "    Operations include:\n",
    "    - reduce\n",
    "    - collect\n",
    "    - count\n",
    "    - save\n",
    "    - lookupKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits\n",
    "\n",
    "RDD maintains a lineage of information\n",
    "Consistency is easy due to immutablility\n",
    "Inexpensive fault tolerance\n",
    "Locality aware scheduling of taks on partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modes of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can be run in \n",
    "\n",
    "    * Client Mode\n",
    "    * Cluster Deploy mode\n",
    "    \n",
    "** Client Mode**\n",
    "\n",
    "    Driver runs on a dedicated server (Master node) inside a dedicated process. This means it has all available resources at it's disposal to execute work.\n",
    "    \n",
    "    Driver opens up a dedicated Netty HTTP server and distributes the JAR files specified to all Worker nodes (big advantage).\n",
    "    \n",
    "    Because the Master node has dedicated resources of it's own, you don't need to \"spend\" worker resources for the Driver program.\n",
    "    \n",
    "    If the driver process dies, you need an external monitoring system to reset it's execution.\n",
    "\n",
    "** Cluster Deploy Mode  **\n",
    "    \n",
    "    Driver runs on one of the cluster's Worker nodes. The worker is chosen by the Master leader\n",
    "    Driver runs as a dedicated, standalone process inside the Worker.\n",
    "    Driver programs takes up at least 1 core and a dedicated amount of memory from one of the workers (this can be configured).\n",
    "    Driver program can be monitored from the Master node using the --supervise flag and be reset in case it dies.\n",
    "    When working in Cluster mode, all JARs related to the execution of your application need to be publicly available to all the workers. This means you can either manually place them in a shared place or in a folder for each of the workers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Running Spark in Local Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the mode by going to spark \"Environment tab\" and checking for spark properties. F\n",
    "\n",
    "* \"Spark.master\" = \"Local[*]\" denotes that it is working on Client mode\n",
    "* \"Spark.master\" = \"YARN\" denotes that it is working on Cluster Deployment Mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the instructions on how to install Spark in Client mode on a single machiine. Inorder to build spark 2 on Ubuntu machine we need\n",
    "\n",
    "* Java Version 8\n",
    "* Install Git \n",
    "* Install Maven\n",
    "* Build Apache Spark 2\n",
    "* Jupyter Notebook\n",
    "\n",
    "* Set Apache Spark Path\n",
    "* Install Find Spark\n",
    "\n",
    "### Java Installation\n",
    "\n",
    "Java Can be Installed in Ubuntu by the following commands.\n",
    "\n",
    ">\\$sudo apt-add-repository ppa:webupd8team/java  \n",
    "> \\$sudo apt-get update  \n",
    "> \\$sudo apt-get install oracle-java7-installer  \n",
    "\n",
    "Check for Java installation by using\n",
    "\n",
    "> \\$java -version\n",
    "\n",
    "It should display the following\n",
    "\n",
    "> \\$java version \"1.7.0_72\"_ Java(TM) SE Runtime Environment (build 1.7.0_72-b14)_    \n",
    "> \\$Java HotSpot(TM) 64-Bit Server VM (build 24.72-b04, mixed mode) \n",
    "\n",
    "\n",
    "### Git Installation\n",
    "If Git is not previously installed in the system, you can install using\n",
    "\n",
    "> \\$sudo apt-get install git\n",
    "\n",
    "### Install Maven\n",
    "Maven can be installed with the following command\n",
    "\n",
    ">\\$sudo apt-get install maven\n",
    "\n",
    "### Build Apache Spark 2\n",
    "\n",
    "Downlodad apache Spark to **/usr/local/share/spark**\n",
    "\n",
    "> \\$mkdir /usr/local/share/spark  \n",
    "> \\$curl http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2.tgz | tar xvz -C /usr/local/share/spark  \n",
    "> \\$cd /usr/local/share/spark/spark-2.0.2\n",
    ">\\$ ./build/mvn -DskipTests clean package\n",
    "\n",
    "Testing if spark is working can be done by\n",
    "\n",
    ">\\$ ./bin/run-example SparkPi 10\n",
    "\n",
    "### Building Jupyter Notebook\n",
    "\n",
    "> \\$ pip3 install --upgrade pip\n",
    "> \\$ pip3 install jupyter\n",
    "\n",
    "### Set Spark Home\n",
    "\n",
    "Open ~/.bashrc and write the following lines  \n",
    "\n",
    "> export SPARK_HOME=/usr/local/share/spark/spark-2.0.2   \n",
    "> export PATH=\\$SPARK_HOME/bin:$PATH  \n",
    "\n",
    "load the bashrc file with  \n",
    "\n",
    "> \\$ source ~/.bashrc  \n",
    "\n",
    "## Install Find Spark\n",
    "\n",
    "> \\$ pip install findspark\n",
    "\n",
    "## Open Jupyter notebook\n",
    "\n",
    "> \\$ jupyter notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Start the spark cluster we can use\n",
    "\n",
    "> sc = pyspark.SparkContext(appName=\"Application\")\n",
    "\n",
    "To Stop spark we can use\n",
    "\n",
    "> sc.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms of Spark\n",
    "\n",
    "Let's quickly go over some important terms:\n",
    "\n",
    "Term                   |Definition\n",
    "----                   |-------\n",
    "RDD                    |Resilient Distributed Dataset\n",
    "Transformation         |Spark operation that produces an RDD\n",
    "Action                 |Spark operation that produces a local object\n",
    "Spark Job              |Sequence of transformations on data with a final action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an RDD\n",
    "\n",
    "There are two common ways to create an RDD:\n",
    "\n",
    "Method                      |Result\n",
    "----------                               |-------\n",
    "`sc.parallelize(array)`                  |Create RDD of elements of array (or list)\n",
    "`sc.textFile(path/to/file)`                      |Create RDD of lines from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Transformations\n",
    "\n",
    "We can use transformations to create a set of instructions we want to preform on the RDD (before we call an action and actually execute them).\n",
    "\n",
    "Transformation Example                          |Result\n",
    "----------                               |-------\n",
    "`filter(lambda x: x % 2 == 0)`           |Discard non-even elements\n",
    "`map(lambda x: x * 2)`                   |Multiply each RDD element by `2`\n",
    "`map(lambda x: x.split())`               |Split each string into words\n",
    "`flatMap(lambda x: x.split())`           |Split each string into words and flatten sequence\n",
    "`sample(withReplacement=True,0.25)`      |Create sample of 25% of elements with replacement\n",
    "`union(rdd)`                             |Append `rdd` to existing RDD\n",
    "`distinct()`                             |Remove duplicates in RDD\n",
    "`sortBy(lambda x: x, ascending=False)`   |Sort elements in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Actions\n",
    "\n",
    "Once you have your 'recipe' of transformations ready, what you will do next is execute them by calling an action. Here are some common actions:\n",
    "\n",
    "Action                             |Result\n",
    "----------                             |-------\n",
    "`collect()`                            |Convert RDD to in-memory list \n",
    "`take(3)`                              |First 3 elements of RDD \n",
    "`top(3)`                               |Top 3 elements of RDD\n",
    "`takeSample(withReplacement=True,3)`   |Create sample of 3 elements with replacement\n",
    "`sum()`                                |Find element sum (assumes numeric elements)\n",
    "`mean()`                               |Find element mean (assumes numeric elements)\n",
    "`stdev()`                              |Find element deviation (assumes numeric elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Execution of Simple Script using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f1d68212c50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using FindSpark and Verify if Spark Context exists for execution\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "sc = sc = pyspark.SparkContext(appName=\"Test Application\")\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stopping PySpark\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding status of Spark Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Spark extension can be used to find the status of the job. \n",
    "\n",
    "> `pip install jupyter-spark`  \n",
    "> `jupyter serverextension enable --py jupyter_spark`  \n",
    "> `jupyter nbextension install --py jupyter_spark`  \n",
    "> `jupyter nbextension enable --py jupyter_spark` \n",
    "> `jupyter nbextension enable --py widgetsnbextension`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "This documents presents how Apache Spark2 Can be integrated with Jupyter Notebook. It also shows how Spark job can be executed on a local machine. The next part would explain how Spark can be installed on a Cloud service such as Google Cloud Compute or Amazon Web services\n",
    "\n",
    "\n",
    "#### <a href=\"http://nbviewer.jupyter.org/github/sreeaurovindh/spark/blob/master/notebook/Behavior%20Modelling%20with%20Apache%20Spark.ipynb\">[Back to Main Page]</a>        <a href=\"http://www.sreeml.com/folio.html\">[Back to Portfolio]</a>   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
